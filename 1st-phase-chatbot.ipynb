{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Natural Language Toolkit (nltk) ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:12.480144Z","iopub.execute_input":"2023-11-09T10:56:12.480536Z","iopub.status.idle":"2023-11-09T10:56:14.263906Z","shell.execute_reply.started":"2023-11-09T10:56:12.480507Z","shell.execute_reply":"2023-11-09T10:56:14.262677Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"user_intent=\"I want 10g fats content, 60g calorie content, 500g sugar content. The dish should be non veg.\"","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:14.266305Z","iopub.execute_input":"2023-11-09T10:56:14.267268Z","iopub.status.idle":"2023-11-09T10:56:14.273082Z","shell.execute_reply.started":"2023-11-09T10:56:14.267219Z","shell.execute_reply":"2023-11-09T10:56:14.271834Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Remove punctuation","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation\n#defining the function to remove punctuation\ndef remove_punctuation(text):\n    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n    return punctuationfree\n#storing the puntuation free text\npunc=remove_punctuation(user_intent)\nprint(punc)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:14.278336Z","iopub.execute_input":"2023-11-09T10:56:14.278791Z","iopub.status.idle":"2023-11-09T10:56:14.287239Z","shell.execute_reply.started":"2023-11-09T10:56:14.278749Z","shell.execute_reply":"2023-11-09T10:56:14.286066Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"I want 10g fats content 60g calorie content 500g sugar content The dish should be non veg\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Convert to lower","metadata":{}},{"cell_type":"code","source":"low=punc.lower()\nprint(low)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:14.931545Z","iopub.execute_input":"2023-11-09T10:56:14.931976Z","iopub.status.idle":"2023-11-09T10:56:14.937858Z","shell.execute_reply.started":"2023-11-09T10:56:14.931943Z","shell.execute_reply":"2023-11-09T10:56:14.936912Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"i want 10g fats content 60g calorie content 500g sugar content the dish should be non veg\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"import re\n\ndef tokenization(text):\n    result = word_tokenize(text)\n    return result\n\n\ntokens = tokenization(low)\nprint(tokens)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:28.468557Z","iopub.execute_input":"2023-11-09T10:56:28.469785Z","iopub.status.idle":"2023-11-09T10:56:28.495348Z","shell.execute_reply.started":"2023-11-09T10:56:28.469713Z","shell.execute_reply":"2023-11-09T10:56:28.494226Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['i', 'want', '10g', 'fats', 'content', '60g', 'calorie', 'content', '500g', 'sugar', 'content', 'the', 'dish', 'should', 'be', 'non', 'veg']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Stopwords Removal","metadata":{}},{"cell_type":"code","source":"#importing nlp library\nimport nltk\n#Stop words present in the library\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords[0:10]\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n#defining the function to remove stopwords from tokenized text\ndef remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output\n#applying the function\nremove_stop=remove_stopwords(tokens)\nprint(remove_stop)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:29.631170Z","iopub.execute_input":"2023-11-09T10:56:29.631697Z","iopub.status.idle":"2023-11-09T10:56:29.642059Z","shell.execute_reply.started":"2023-11-09T10:56:29.631656Z","shell.execute_reply":"2023-11-09T10:56:29.640814Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['want', '10g', 'fats', 'content', '60g', 'calorie', 'content', '500g', 'sugar', 'content', 'dish', 'non', 'veg']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### POS- Tagging","metadata":{}},{"cell_type":"code","source":"final = nltk.pos_tag(tokens)\nprint(final)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:30.519330Z","iopub.execute_input":"2023-11-09T10:56:30.520458Z","iopub.status.idle":"2023-11-09T10:56:30.724872Z","shell.execute_reply.started":"2023-11-09T10:56:30.520411Z","shell.execute_reply":"2023-11-09T10:56:30.723591Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[('i', 'NN'), ('want', 'VBP'), ('10g', 'CD'), ('fats', 'NNS'), ('content', 'JJ'), ('60g', 'CD'), ('calorie', 'NN'), ('content', 'NN'), ('500g', 'CD'), ('sugar', 'NN'), ('content', 'NN'), ('the', 'DT'), ('dish', 'NN'), ('should', 'MD'), ('be', 'VB'), ('non', 'JJ'), ('veg', 'NN')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Adjective-Noun Extraction\nThis code uses NLTK to extract noun phrases from tagged text based on a defined grammar pattern. It identifies pairs of adjectives/CD and nouns or hyphenated nouns within the extracted noun phrases and prints them.","metadata":{}},{"cell_type":"code","source":"\nfrom nltk.tokenize import word_tokenize\nfrom nltk.chunk import RegexpParser  # Import RegexpParser\n\n\n\n# Define a grammar pattern for NP (noun phrase) that consists of an optional adjective (JJ) or cardinal number (CD) followed by a noun (NN) or a hyphenated noun (NN-NN)\ngrammar = r'NP: {<JJ|CD>?<NN|NNS>-<NN|NNS>|<JJ|CD>?<NN|NNS>}'\n\n# Create a RegexpParser with the defined grammar\nchunk_parser = RegexpParser(grammar)\n\n# Parse the tagged text to extract noun phrases\ntree = chunk_parser.parse(final)\n\n# Initialize an empty list to store the adjective/CD-noun or hyphenated noun pairs\npairs = []\n\n# Traverse the parse tree to find noun phrases and extract adjectives/CD and nouns or hyphenated nouns\nfor subtree in tree.subtrees():\n    if subtree.label() == 'NP':\n        # Get the words in the noun phrase\n        words = [word for word, pos in subtree.leaves()]\n        # Find and extract the adjective/CD (if it exists)\n        adj_cd = [word for word, pos in subtree.leaves() if pos in ['JJ', 'CD']]\n        # Find and extract the noun or hyphenated noun\n        noun = ' '.join(word for word, pos in subtree.leaves() if pos in ['NN', 'NNS'])\n        if noun:\n            # If adjective/CD and noun or hyphenated noun are found, add them to the pairs list\n            if adj_cd:\n                pairs.append((adj_cd, noun))\n            else:\n                # If there is no adjective or CD, add a placeholder (None)\n                pairs.append((None, noun))\n\n# Now, pairs contains the pairs of adjectives/CD or hyphenated nouns with nouns\nfor pair in pairs:\n    print(pair)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:31.569428Z","iopub.execute_input":"2023-11-09T10:56:31.569876Z","iopub.status.idle":"2023-11-09T10:56:31.581773Z","shell.execute_reply.started":"2023-11-09T10:56:31.569842Z","shell.execute_reply":"2023-11-09T10:56:31.580447Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(None, 'i')\n(['10g'], 'fats')\n(['60g'], 'calorie')\n(None, 'content')\n(['500g'], 'sugar')\n(None, 'content')\n(None, 'dish')\n(['non'], 'veg')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Converting to input form\nThe code extracts nutritional information and veg-nonveg status from pairs, updating a dictionary and printing the results.","metadata":{}},{"cell_type":"code","source":"# Initialize your dictionary with keys and set their values to None\nnutrition_data = {\n    \"Calories\": None,\n    \"FatsContent\": None,\n    \"CholesterolContent\": None,\n    \"SodiumContent\": None,\n    \"CarbohydrateContent\": None,\n    \"FiberContent\": None,\n    \"SugarContent\": None,\n    \"ProteinContent\": None,\n    \"veg-nonveg\": None,\n    \"ingredients\": None\n}\n\n\n# Variable to track if \"veg\" or \"non\" is found in the pairs\nveg_found = False\nnon_found = False\n\n# Update the dictionary values based on the pairs\nfor adjective_cd, noun in pairs:\n    if noun and adjective_cd:\n        noun_lower = noun.lower()  # Convert to lowercase for case-insensitive comparison\n        for key in nutrition_data.keys():\n            if noun_lower in key.lower():\n                nutrition_data[key] = adjective_cd[0]\n        if \"veg\" in noun_lower:\n            veg_found = True\n        elif \"non\" in noun_lower:\n            non_found = True\n\n# Update \"veg-nonveg\" based on the presence of \"veg\" or \"non\"\nif veg_found and not non_found:\n    nutrition_data[\"veg-nonveg\"] = \"veg\"\nelif non_found and not veg_found:\n    nutrition_data[\"veg-nonveg\"] = \"nonveg\"\n\n# Print the updated dictionary\nprint(\"Updated Nutrition Data:\")\nfor key, value in nutrition_data.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:56:32.510923Z","iopub.execute_input":"2023-11-09T10:56:32.511327Z","iopub.status.idle":"2023-11-09T10:56:32.522710Z","shell.execute_reply.started":"2023-11-09T10:56:32.511296Z","shell.execute_reply":"2023-11-09T10:56:32.521914Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Updated Nutrition Data:\nCalories: 60g\nFatsContent: 10g\nCholesterolContent: None\nSodiumContent: None\nCarbohydrateContent: None\nFiberContent: None\nSugarContent: 500g\nProteinContent: None\nveg-nonveg: veg\ningredients: None\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}